{"name":"Xke-kafka","tagline":"","body":"# Installation\r\n\r\n## Download et installation\r\n\r\nTélécharger la version  0.8.2.1 de Kafka [http://kafka.apache.org/downloads.html](http://kafka.apache.org/downloads.html) \r\n(Si vous voulez utiliser Scala avec Kafka assurez vous de prendre la version qui correspond à la version de Scala).\r\n\r\nDans un répertoire de travail (ex : ~/xke-kafka) \r\n\r\nExtraire la distribution de Kafka\r\n\r\n```\r\n$ tar xzf kafka_2.10-0.8.2.1.tgz\r\n```\r\n\r\nCréer deux répertoires qui contiendront les logs des différents brokers de Kafka :\r\n\r\n```\r\n$ mkdir log1\r\n$ mkdir log2\r\n```\r\n\r\n\r\n### Démarrer Zookeeper\r\n\r\nKafka contient sa propre distribution de Zookeeper (pour les tests et les demos, NE PAS UTILISER EN PROD)\r\n\r\nDémarrer Zookeeper :\r\n\r\n    ./bin/zookeeper-server-start.sh config/zookeeper.properties &\r\n\r\n## Démarrer un broker\r\n\r\nEditer le fichier de configuration de Kafka config/server.properties et modifier la valeur de log.dirs :\r\n\r\n    log.dirs=~/xke-kafka/log1\r\n\r\nDémarrer le broker :\r\n\r\n    ./bin/kafka-server-start.sh config/server.properties &\r\n\r\n## Démarrer un second broker\r\n\r\nCopier le fichier de configuration de Kafka\r\n\r\n    cp config/server.properties config/server2.properties\r\n\r\nDans la configuration du second broker (config/server2.properties), modifier les valeurs de broker.id, \r\nport et log.dirs :\r\n\r\n    broker.id=1\r\n    port=9091\r\n    log.dirs=~/xke-kafka/log2\r\n\r\nDémarrer le second broker :\r\n\r\n    ./bin/kafka-server-start.sh config/server2.properties &\r\n\r\n## Créer une topic\r\n\r\nKafka propose un utilitaire pour la gestion des topics. \r\nLancer le pour découvrir ses options : \r\n\r\n    ./bin/kafka-topics.sh\r\n\r\nPuis démarrer la topic first : \r\n\r\n    ./bin/kafka-topics.sh --create \\\r\n      --zookeeper localhost:2181 \\\r\n      --replication-factor 2 \\\r\n      --partitions 2 \\\r\n      --topic first\r\n\r\nVerifier les partitions de cette topic :\r\n\r\n    ./bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic first\r\n\r\n## Valider la configuration avec le console-producer et le console-consumer\r\n\r\nDans un nouveau terminal, créer un producer kafka qui enverra dans la topic tous messages écrits dans la console :\r\n\r\n    ./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic first\r\n\r\nDans un autre terminal, lancer un consommateur kafka qui affichera dans la console tous les messages d'une topic :\r\n\r\n    ./bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic first --from-beginning\r\n\r\nVérifier que tout marche bien.\r\n\r\n# Créer un Producer Kafka\r\n\r\n## Old producers\r\n\r\n- Sync -> Safe but slow\r\n- Async -> high performance but silent errors\r\n\r\npour l'instant on ne l'implémente pas, faire un résumé de l'API et des pros and cons\r\n\r\n## New producers\r\n\r\n- can be use synchronously or asynchronously\r\n- both modes can handle errors\r\n- bounded memory usage\r\n- multi-threaded\r\n\r\nFaire implémenter un producer, et verifier que ça marche avec le console-consumer\r\n\r\n\r\n# Consumer\r\n\r\n## High Level Consumer\r\n\r\nRésumé des concepts et de l'API, pros and cons\r\n\r\nFaire implémenter un consumer, et vérifier qu'il recoit les données envoyées par le producer ci-dessus\r\n\r\n# Kafka par la face nord\r\n\r\n## Notions\r\n\r\nPetit rappel des notions de base:\r\n\r\n* **publish/subscribe**: pattern d'architecture qui découple production et consommation. Un émetter poste un message dans un broker. On peut avoir la distribution de ce message de 0 à N listeners connectés. C'est le listener qui choisit ce qu'il reçoit.\r\n* **noeud**: un serveur Kafka.\r\n* **cluster**: un ensemble de noeuds Kafka communicant entre eux pour former un service cohérent. Les noeuds se synchronisent au travers de Zookeeper pour se répartit les données et les réplicats.\r\n* **Zookeeper**: peut être vu que une base de registre distribuée hautement disponible et fortement cohérente. Un des outils incontournables dans les architectures master/master.\r\n* **topic**: un topic est un nom logique regroupant des partitions. On stocke dans un topic des messages du même type, facilitant ainsi la consommation par les listeners. Un topic possède des caractéristiques comme le nombre de partitions, le délai de rétention des messages (Kafka fait de la purge automatique), le facteur de réplication...\r\n* **partition**: une partition peut être vue comme une pile de message. Mais contraitement à une queue, ce n'est pas une FIFO. Les messages sont historisés pendant pour la durée de rétention du topic. Ils ne disparaissent pas à la consommation. L'ordre des messages est seulement garanti au sein d'une partition, pas d'un topic. Il n'y a toujours au plus qu'un noeud Kafka leader sur une partition. Il centralise ainsi toutes les écritures. Deux messages envoyés successivement par un même producteur seront stockés dans le même ordre dans la partition.\r\n* **offset**: ID unique d'un message pour un couple topic/partition. C'est un nombre strictement croissant. C'est le point central de la production/consommation de message dans Kafka\r\n* **producteur**: c'est l'émetteur d'un message. Il peut poster un message dans un topic en choississant spécifiquement une partition ou un au hasard.\r\n* **consommateur**: c'est l'écouteur de messages. Il se connecte à un topic/partition et demande à recevoir des messages. Il existe deux API: HighLevel and SimpleConsumer. La première est très simple à mettre en place mais il y a peu de paramètres de gestion de l'offset. La seconde offre un contrôle total au prix de quelques bouts de code à faire. On peut choisir de consommer des messages depuis le premier offset d'une partition, du dernier pour ne recevoir que les suivants, ou d'un offset arbitraire.\r\n* **groupe de consommateur**: c'est un ensemble fonctionnel de consommateurs. Les partitions dans Kafka sont persistentes. Les messages ne disparaissent pas à la consommation. Pourtant, les consommateurs de messages ne veulent pas forcément traiter tous les messages debuit le début de la partition à chaque redémarrage. Un consommateur peut faire un *commit* de son dernier offset lu dans Kafka. Au redémarrage, il suffit d'aller chercher cette valeur est de redémarrer la consommation depuis cet offset. Cet offset est au moins unique pour un couple topic/partition. Mais comme plusieurs consommateurs peuvent lire la même partition du même topic en même temps, et à des vitesses différentes, les offsets sont commités pour le triplet (groupe,topic,partittion).\r\n\r\n## Les étapes pour créer un consommateur\r\n### Récupération de la configuration du cluster pour un topic\r\n\r\nToute la configuration du cluster est mise à jour par Kafka dans Zookeeper. Pour pouvoir consommer des messages, il faut récupérer dans les metadata du topic le nombre de partitions configurés. On rappelle qu'un topic n'est qu'un ensemble de partition, chaque partition étant une \"file\" de messages persistante.\r\n\r\nIl faut:\r\n\r\n* créer un client Zookeeper\r\n* Chercher dans kafka.admin.AdminUtils la bonne méthode\r\n\r\n```\r\n    // Réponse\r\n    val topicMetadata = AdminUtils.fetchTopicMetadataFromZk(topic, zkClient)\r\n```\r\n\r\n \t\r\n ### Se connecter à une partition\r\n\r\nIl existe à un instant au plus 1 noeud Kafka leader pour une partition d'un topic donné. \r\nPour faire simple, nous allons nous connecter à toutes les partitions du topic en une fois. Il faudra donc faire une boucle sur la liste des partitions que vous avez récupérer précédemment.\r\n\r\nIl faut: \r\n\r\n* fouiller dans la réponse précédente pour trouver chaque leader de chaque partition. \r\n* se connecter au broker en instantiant un kafka.consumer.SimpleConsumer par partition.\r\n\r\n```\r\n    // Réponse\r\n    val partitionsBroker: Map[Int, Option[Broker]] = topicMetadata.partitionsMetadata.groupBy(_.partitionId).toMap.mapValues(_.head.leader)\r\n\r\n    val partitionsConsumer: Map[Int, Option[SimpleConsumer]] = partitionsBroker.mapValues{\r\n        optionalBroker => optionalBroker.map{leader => new SimpleConsumer(leader.host, leader.port, 10000, 64000, \"aCLientId\")}\r\n    }\r\n```\r\n\r\n### Trouver l'offset de démarrage de consommation\r\n\r\nUne partition est un journal en ajout seulement. Chaque message possède un numéro unique au sein d'une même partition. Cet identifiant, issu d'un compteur monotonique (strictement croissant), est nommé offset. \r\nPour chaque requête de données à Kafka, on lui précise le nombre de messages que l'on veut recevoir, et depuis quelle position, offset.\r\n\r\nIl faut: \r\n\r\n* trouver sur SimpleConsumer une méthode nous permettant de trouver l'identifiant du premier offset connu de chaque partition.\r\n\r\n```\r\n\t//Réponse\r\n\tconsumer.earliestOrLatestOffset(topicAndPartition, OffsetRequest.EarliestTime, Request.OrdinaryConsumerId)\t\r\n```\r\n\r\n\r\nNB: on pourrait aussi lancer le consommateur depuis la fin courante de la file. Ainsi, le consommateur ne recevrait de messages que lorsqu'un nouveau serait posté.\r\n\r\n### Faire une requête de données\r\n\r\nMaintenant que nous avons la connexion au leader et l'offset à demander, il n'y a plus qu'à récupérer les infos. Dans Kafka, on ne demande pas N messages. On demande une taille à récupérer. Dans la réponse, nous aurons ensuite un itérateur permettant de parcourir chaque message reçu. Il est donc **important** de connaître la taille des messages que l'on manipule. Cela semble bizarre au début mais cela se révèle être un atout majeur en terme de performance. En effet, toutes les I/O se mesurent en Bytes, network, buffer, disque... en ne manipulant que des tailles en bytes, il est ainsi d'être le plus précis possible pour le tuning de performance.\r\n\r\nIl faut\r\n\r\n* créer une FetchRequest grâce au FetchRequestBuilder. \r\n* l'exécuter avec le SimpleConsumer\r\n* itérer sur l'Iterator de MessageSet \r\n```\r\n    //Réponse\r\n    val request = new FetchRequestBuilder()\r\n        .clientId(groupId)\r\n        .addFetch(topic, partitionId, nextOffsetToFetch, maxMessageSize * count)\r\n        .maxWait(fetchTimeout)\r\n        .build()\r\n    val fetchReply = consumer.fetch(request)\r\n```   \r\n\r\nNB: il est possible que Kafka vous envoie des messages un peu avant l'offset qui est demandé (pour des raisons d'optimisation). Si le côté transactionnel est important pour vous, pensez à filtrer sur les offsets des messages reçus.   \r\n\r\n\r\n### Commit\r\n\r\nVous l'aurez ainsi remarqué, c'est le consommateur qui a la responsabilité de maintenir l'offset de lecture. Le broker Kafka ne sait pas à priori qui a déjà consommé quoi.\r\nIl existe deux façons proposées par Kafka pour maintenir cette information, mais vous pouvez utiliser la votre. Il suffit juste de maintenir quelque part ce fameux offset de consommation.\r\nInitialement, Kafka stockait les offsets dans Zookeeper. Cette solution fortement cohérente en système distribué s'est avéré trop peu performante. \r\nLa seconde solution proposée par Kafka est de stocké lui même l'offset dans un topic maintenu par le cluster. Il existe un noeud particulier dans le cluster qui joue le rôle du coordinateur à qui on peut demander les offsets et de \"commiter\" un offset pour un groupe de consommateurs, topic et partition.\r\n\r\nPour trouver ce coordinateur\r\n\r\nIl faut:\r\n\r\n* boucler sur la liste des brokers et s'arrêter au premier qui fonctionne (ou recommencer jusqu'à ce que cela fonctionne)\r\n* créer un blockingChannel sur un broker\r\n    val channel = new BlockingChannel(host, port, bufferSize, bufferSize, socketTimeout)\r\n    channel.connect()\r\n* Faire une requête ConsumerMetadataRequest \r\n    channel.send(new ConsumerMetadataRequest(groupId))\r\n    val reply = ConsumerMetadataResponse.readFrom(channel.receive().buffer)\r\n* S'il existe un coordinateur, il faut s'y connecter\r\n* Faire un commit \r\n```\r\n    val request = OffsetCommitRequest(\r\n        groupId,\r\n\tMap(topicAndPartition -> OffsetAndMetadata(offset)),\r\n\tversionId = 1\r\n    )\r\n    println(s\"Committing offset <$offset> to partition <$partition>:<$groupId>\")\r\n    val reply = Try {\r\n        channel.send(request)\r\n\tOffsetCommitResponse.readFrom(channel.receive().buffer)\r\n    }\r\n    reply.map(_.commitStatus(topicAndPartition)).filter(_ == NoError)\r\n```\r\n\r\nPour lire cette valeur et ainsi recommencer à lire depuis le dernier offset connu, il faut :\r\n\r\n* sur le channel du coordinateur, faire une requête OffsetFetchRequest\r\n\r\n```\r\nval request = OffsetFetchRequest(groupId, List(topicAndPartition))\r\nchannel.send(request)\r\nOffsetFetchResponse.readFrom(channel.receive().buffer)\r\n```\r\n\r\nVous pouvez ainsi récupérer le dernier offset connu, à la prochaine requête, vous pourez utiliser cette valuer.\r\n\r\n## Et ce n'est pas fini!\r\n\r\nIl manque encore plein de choses dans cette implem. Le cluster est dynamique, le coordinateur peut changer de noeud, les partitions peuvent être réassignées sur un autre noeud. Il faut\r\n\r\n* Écouter les événements depuis ZK pour suivre les assignements des partitions\r\n* Réessayer plusieurs fois certaines action quand le cluster n'est pas stable (en phase de transition)\r\n* Il se peut que'offset commité n'existe plus dans Kafka, il faut ainsi s'assurer qu'il existe supérieur au premier offset connu...","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}